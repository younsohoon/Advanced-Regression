---
title: "Statistical Analysis - Random Forest"
author: "Sohoon Youn"
date: "2023-04-13"
output: html_document
---

## 3-a. Identify the importance variables
```{r inclue = TRUE, warning=FALSE}
set.seed(777)
fit = randomForest(los ~ ., data = dat, importance=TRUE)

imp=data.frame(importance(fit))
ord <- order(imp$X.IncMSE, decreasing=TRUE)
imp=imp[ord, ]
imp2 = imp[1:10,]
imp2
```
The importance of each variable is measured by the percentage increase in the mean squared error (MSE) when that variable is removed from the model. According to the above table, `weight`, `height`, `admission_location`, and `heartrate_mean` show the significantly greater values in X.IncMSE. However, `hematocrit_min`, `hemoglobin_min`, `hematocrit_max`, `aniongap` have similar values in X.IncMSE but `aniongap` has a significant value in IncNodePurity.

Therefore, `weight`, `height`, `heartrate_mean`, `aniongap`, `admission_location` are added to the random forests. 

## 3-b. Model matrix for hyperparameters 
```{r inclue = TRUE, warning=FALSE}
X = model.matrix(~ weight + height + heartrate_mean + aniongap + admission_location, data=dat)
Y = dat$los
```
The model matrix is built to run rfcv(), which is a cross validation function for random forests. 

```{r inclue = TRUE, warning=FALSE}
hyperparams = expand.grid(mtry = c(15,5,3,1), ntree=c(1000), 
                          maxnodes=c(10,100,300), nodesize=c(1,20,50), sampsize=c(250,300,350))
hyperparams[1:10,]
```

The ranges for each hyperparameter are randomly chosen. Those hyperparameters will be tuned by cross-validation based on hyperparagrams. A random forest is fitted with all the combinations of hyperparameters according to the hyperparams, then the hyperparameters with the least mean squared error will be fitted to the final model.

## 3-c. Tuned hyperparameters by 10-fold Cross-Validation
```{r inclue = TRUE, warning=FALSE, eval=FALSE}
result = data.frame()

for (i in 1:nrow(hyperparams)) {
  set.seed(777)
  model = rfcv(X, Y, cv.fold = 10, mtry = function(x){hyperparams$mtry[i]},
               ntree=hyperparams$ntree[i], maxnodes=hyperparams$maxnodes[i],
               nodesize=hyperparams$nodesize[i], sampsize=hyperparams$sampsize[i])
  tem = list(n.var = model$n.var, error.cv = model$error.cv)
  tem2 = c(mse = min(tem$error.cv), mtry = hyperparams$mtry[i],
           ntree=hyperparams$ntree[i],maxnodes=hyperparams$maxnodes[i],
           nodesize=hyperparams$nodesize[i], sampsize=hyperparams$sampsize[i])
  print(tem2)
  result = rbind(result, tem2)
}
colnames(result) = c('mse','mtry','ntree','maxnodes','nodesize','sampsize')
head(result)
optimalFeatures = c(result$mtry[which.min(result$mse)],result$ntree[which.min(result$mse)],
                    result$maxnodes[which.min(result$mse)],
                    result$nodesize[which.min(result$mse)],
                    result$sampsize[which.min(result$mse)])
optimalFeatures 
```

The list of hyperparameters to be tuned :

* mtry = Number of variables randomly sampled as candidates at each split
* ntree = Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times.
* maxnodes = Maximum number of terminal nodes trees in the forest can have. 
* nodesize = Minimum size of terminal nodes.
* sampsize = Size(s) of sample to draw.

The ranges for each hyperparameter are randomly chosen, and those hyperparameters are tuned by 10-fold cross-validation. A random forest is fitted with all the combinations of hyperparameters according to the hyperparams, then the hyperparameters with the least mean squared error are chosen to the the final model.

## 3-d. Optimized random forest 
```{r inclue = TRUE, warning=FALSE}
rf = randomForest(los ~ weight + height + heartrate_mean + aniongap + admission_location, 
                   mtry = 3, ntree=1000, maxnodes= 300,
                   nodesize = 20, sampsize = 350,
                   data = dat, importance=TRUE)
```

tuned hyperparameters :

* mtry: 3
* ntree: 1000
* maxnodes = 300
* nodesize = 20
* sampsize = 350

## 3-e. Model evaluation using K-fold cross-validation
```{r inclue = TRUE, warning=FALSE}
kfold = function(n){
  data = dat
  k = 10
  predictors = names(data)[2:ncol(data)]
  response = names(data)[1]
  
  folds = createFolds(data$los, k)
  dat = data[-folds[[n]], ]
  dtest = data[folds[[n]], ]
  
  set.seed(777) 
  fit = randomForest(los ~ weight + height + heartrate_mean + aniongap + admission_location, 
                   mtry = 3, ntree=1000, maxnodes= 300,
                   nodesize = 20, sampsize = 350,
                   data = dat) # rf
  pred = predict(fit, newdata=dtest)
  res = data.frame(Id=folds[[n]], los=pred)
  return(sqrt(mean((res$los - dtest$los)^2)))
}

cv_score = 0
for (i in 1:10){
   cv_score = kfold(i) + cv_score
 }
cat("According to the 10-fold cross-validation, 
    the MSE of an optimized randomForest is approximately ", cv_score/10)
```


###  Partial Dependence Plots (PDP)

```{r echo = FALSE}
par(mfrow=c(2,2))
partialPlot(rf,dat,"weight",cex.main=0.6,asp=0.5)
partialPlot(rf,dat,"height",cex.main=0.6)
partialPlot(rf,dat,"heartrate_mean",cex.main=0.6)
partialPlot(rf,dat,"aniongap",cex.main=0.6)
```

Four partial dependence plots (PDP) are drawn. They give a graphical depiction of the marginal effect of a variable on the response. The effect of a variable is measured in change in the mean response. PDP assumes independence between the feature for which is the PDP computed and the rest. 

The y-axis of a partial dependence plot shows the average predicted response for each value of the feature after adjusting for other features in the model. If partial dependence is high, it means that there is a strong relationship between that feature and the response.

Four significant variables including, `weight`, `height`, `heartrate_mean`, `aniongap`, are with high partial dependence at certain value. Particularly, `weight` < 3, `height` < 40, `heartrate_mean` > 140, and `aniongap` < 15 or `aniongap` > 25 have the strong relationship with response.

### First tree of Random forest

```{r echo = FALSE}
# extract a single tree
tree1 = getTree(rf, 1, labelVar=TRUE)

left_daughter = c(tree1$`left daughter`[1:20])
right_daughter = c(tree1$`right daughter`[1:20])
split_var = c(tree1$`split var`[1:20])
split_point = c(tree1$`split point`[1:20])
prediction = c(tree1$prediction[1:20])

pval=data.frame(left_daughter,right_daughter,split_var,split_point,prediction)
kable(pval) %>% 
  kable_styling(position = "center", latex_options = "HOLD_position")
```

    * `left daughter`: The index of the left daughter node

    * `right daughter`: The index of the right daughter node

    * `split var`: The index of the variable used to split the node

    * `split point`: The value of the split variable

    * `prediction`: The predicted value for terminal nodes


The first tree is extracted from a random forest. The rows explain nodes from top to bottom of a single tree. According to the first row of the table, the first node has a left daughter with index 2 and a right daughter with index 3. This node is split by the variable `weight` with the value 1.34. If `weight` is greater than 1.34, then the data passes down to left node; otherwise, it passes down to the right node. Lastly, the response variable is predicted based on this node, which is `prediction`. 


### Visualization

```{r echo = FALSE, warning=FALSE}

# # extract a single tree
# set.seed(777)
# rf2 = rfsrc(los ~ weight + height + heartrate_mean + aniongap + admission_location,
#                    mtry = 3, ntree=1000, maxnodes= 300,
#                    nodesize = 20, sampsize = 350,
#                    data = dat,)
# tree = get.tree.rfsrc(rf2, tree.id=1)
# plot(tree)

rf_model = rf

rpart_tree = rpart::rpart(as.formula(rf_model$call$formula), 
                           data = dat, 
                           control = rpart::rpart.control(cp = 0), 
                           model = FALSE)

# plot tree using rpart.plot
rpart.plot(rpart_tree, main = "Random Forest Tree Diagram")
```

A single tree from the random forest is created. Based on the top node of the tree diagram, the data passes down to the left node if `height` is greater than or equal to 39. Then, the data is split by `weight`, `height`, `aniongap`, `admission_location` based on the certain values of each variable.

This information is not directly related to our model interpretation. This is the only a single tree out of 1000 trees in a random forest. While each decision tree in random forests is easy to interpret, the ensemble of trees is more of a black box. This is because the output of a random forest is an average of the outputs of all the individual trees, which can make it difficult to understand how each variable contributes to the final prediction. Therefore, it can be difficult to interpret the model.
